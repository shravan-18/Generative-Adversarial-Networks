{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7076104,"sourceType":"datasetVersion","datasetId":4075734}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Controlled Generation in GANs**","metadata":{}},{"cell_type":"markdown","source":"## **Dataset Example**","metadata":{}},{"cell_type":"markdown","source":"![Example](https://mmlab.ie.cuhk.edu.hk/projects/CelebA/overview.png)","metadata":{}},{"cell_type":"markdown","source":"# **Import Dependencies**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torchvision\nfrom torch import nn\nfrom tqdm.auto import tqdm\nfrom torchvision import transforms\nfrom torchvision.utils import make_grid\nfrom torchvision.datasets import CelebA\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device being use: {device}\")\n\ndef show_tensor_images(image_tensor, num_images=16, size=(3,64,64), nrow=3):\n    \n    image_tensor = (image_tensor+1)/2\n    image_unflat = image_tensor.detach().cpu()\n    image_grid = make_grid(image_unflat[:num_images], nrow=nrow)\n    plt.imshow(image_grid.permute(1,2,0).sqeeze())\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-28T17:19:54.097776Z","iopub.execute_input":"2023-11-28T17:19:54.098122Z","iopub.status.idle":"2023-11-28T17:19:57.410292Z","shell.execute_reply.started":"2023-11-28T17:19:54.098094Z","shell.execute_reply":"2023-11-28T17:19:57.409316Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Device being use: cuda\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Generator**","metadata":{}},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, z_dim=10, im_chan=3, hidden_dim=64):\n        super().__init__()\n        self.z_dim = z_dim\n        self.gen = nn.Sequential(\n            self.make_gen_block(z_dim, hidden_dim*8),\n            self.make_gen_block(hidden_dim*8, hidden_dim*4),\n            self.make_gen_block(hidden_dim*4, hidden_dim*2),\n            self.make_gen_block(hidden_dim*2, im_chan, kernel_size=4, final_layer=True)\n        )\n        \n    def make_gen_block(self, input_channels, output_channels, kernel_size=3, stride=2, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.ReLU(inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.ConvTranspose2d(input_channels, output_channels, kernel_size, stride),\n                nn.Tanh(),\n            )\n        \n    def forward(self, noise):\n        x = noise.view(len(noise), self.z_dim, 1, 1)\n        return self.gen(x)\n    \ndef get_noise(n_samples, z_dim, device):\n    return torch.randn(n_samples, z_dim).to(device)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T17:20:55.622289Z","iopub.execute_input":"2023-11-28T17:20:55.623367Z","iopub.status.idle":"2023-11-28T17:20:55.632331Z","shell.execute_reply.started":"2023-11-28T17:20:55.623333Z","shell.execute_reply":"2023-11-28T17:20:55.631381Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# **Classifier**","metadata":{}},{"cell_type":"code","source":"class Classifier(nn.Module):\n    def __init__(self, im_chan=3, n_classes=2, hidden_dim=64):\n        super().__init__()\n        self.classifier = nn.Sequential(\n            self.make_classifier_block(im_chan, hidden_dim),\n            self.make_classifier_block(hidden_dim, hidden_dim*2),\n            self.make_classifier_block(hidden_dim*2, hidden_dim*4, stride=3),\n             self.make_classifier_block(hidden_dim*4, n_classes, final_layer=True),\n        )\n        \n    def make_classifier_block(self, input_channels, output_channels, kernel_size=4, stride=2, final_layer=False):\n        if final_layer:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(input_channels, output_channels, kernel_size, stride),\n                nn.BatchNorm2d(output_channels),\n                nn.LeakyReLU(0.2, inplace=True),\n            )\n        \n    def forward(self, image):\n        class_pred = self.classifier(image)\n        return class_pred.view(len(class_pred), -1)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T17:20:02.306420Z","iopub.execute_input":"2023-11-28T17:20:02.306800Z","iopub.status.idle":"2023-11-28T17:20:02.315285Z","shell.execute_reply.started":"2023-11-28T17:20:02.306770Z","shell.execute_reply":"2023-11-28T17:20:02.314354Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## **Parameter Specifications**","metadata":{}},{"cell_type":"code","source":"z_dim = 64\nbatch_size = 128","metadata":{"execution":{"iopub.status.busy":"2023-11-28T17:20:04.724177Z","iopub.execute_input":"2023-11-28T17:20:04.724522Z","iopub.status.idle":"2023-11-28T17:20:04.728592Z","shell.execute_reply.started":"2023-11-28T17:20:04.724494Z","shell.execute_reply":"2023-11-28T17:20:04.727663Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# **Train Classifier**","metadata":{}},{"cell_type":"code","source":"def train_classifier(filename):\n    # Target all the classes, so that's how many the classifier will learn\n    label_indices = range(40)\n    \n    n_epochs = 10\n    lr = 0.001\n    beta_1 = 0.5\n    beta_2 = 0.999\n    image_size = 64\n    \n    transform = transforms.Compose([\n        transforms.Resize(image_size),\n        transforms.CenterCrop(image_size),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    ])\n    \n    dataloader = DataLoader(\n        CelebA(\".\", split=\"train\", download=True, transform=transform),\n        batch_size=batch_size,\n        shuffle=True\n    )\n    \n    classifier = Classifier(n_classes=len(label_indices)).to(device)\n    class_opt = torch.optim.Aam(classifier.parameters(), lr=lr, betas=(beta_1, beta_2))\n    loss = nn.BCEWithLogitsLoss()\n    \n    classifier_losses = []\n    \n    for epoch in range(n_epochs):\n        curr_epoch_loss = 0\n        for real, labels in tqdm(dataloader):\n            real = real.to(device)\n            labels = labels[:, label_indices].to(device).float()\n            \n            class_opt.zero_grad()\n            pred_labels = classifier(real)\n            classifier_loss = loss(pred_labels, labels)\n            classifier_loss.backward()\n            class_opt.step()\n            \n            curr_epoch_loss += classifier_loss.item()\n            \n        print(f\"Current epoch: {epoch}/{n_epochs}\")\n        classifier_losses += [curr_epoch_loss/len(dataloader)]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Load Model**","metadata":{}},{"cell_type":"code","source":"gen_path = '/kaggle/input/gan-data/pretrained_celeba.pth'\nclass_path = '/kaggle/input/gan-data/pretrained_classifier.pth'\n\ngen = Generator(z_dim).to(device)\ngen_dict = torch.load(gen_path, map_location=torch.device(device))[\"gen\"]\ngen.load_state_dict(gen_dict)\ngen.eval()\n\nn_classes = 40\nclassifier = Classifier(n_classes=n_classes).to_device()\nclass_dict = torch.load(class_path, map_location=torch.device(device))[\"classifier\"]\nclassifier.load_state_dict(class_dict)\nclassifier.eval()\n\nprint(\"Loaded Generator and Classifier successfully!\")\n\nopt = torch.optim.Adam(classifier.parameters(), lr=0.01)","metadata":{"execution":{"iopub.status.busy":"2023-11-28T17:20:59.287565Z","iopub.execute_input":"2023-11-28T17:20:59.287940Z","iopub.status.idle":"2023-11-28T17:21:02.401459Z","shell.execute_reply.started":"2023-11-28T17:20:59.287912Z","shell.execute_reply":"2023-11-28T17:21:02.400140Z"},"trusted":true},"execution_count":7,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m class_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/kaggle/input/gan-data/pretrained_classifier.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m gen \u001b[38;5;241m=\u001b[39m Generator(z_dim)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 5\u001b[0m gen_dict \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgen_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgen\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      6\u001b[0m gen\u001b[38;5;241m.\u001b[39mload_state_dict(gen_dict)\n\u001b[1;32m      7\u001b[0m gen\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/serialization.py:1033\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(f, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m<\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m   1028\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1029\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load does not work with file-like objects that do not implement readinto on Python 3.8.0 and 3.8.1. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1030\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReceived object of type \u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(f)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m. Please update to Python 3.8.2 or newer to restore this \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1031\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunctionality.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1033\u001b[0m magic_number \u001b[38;5;241m=\u001b[39m \u001b[43mpickle_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m magic_number \u001b[38;5;241m!=\u001b[39m MAGIC_NUMBER:\n\u001b[1;32m   1035\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid magic number; corrupt file?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mUnpicklingError\u001b[0m: invalid load key, '<'."],"ename":"UnpicklingError","evalue":"invalid load key, '<'.","output_type":"error"}]},{"cell_type":"markdown","source":"# **Training Controlled Generation**\n\n## **Equation for Gradient Ascent: new = old + (âˆ‡ old * weight)**","metadata":{}},{"cell_type":"code","source":"def calculate_updated_noise(noise, weight):\n    new_noise = noise + (noise.grad * weight)\n    return new_noise","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check that it works for generated images\nopt.zero_grad()\nnoise = get_noise(32, z_dim).to(device).requires_grad_()\nfake = gen(noise)\nfake_classes = classifier(fake)[:, 0]\nfake_classes.mean().backward()\nnoise.data = calculate_updated_noise(noise, 0.01)\nfake = gen(noise)\nfake_classes_new = classifier(fake)[:, 0]\nassert torch.all(fake_classes_new > fake_classes)\nprint(\"Success!\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_images = 8\nfake_image_history = []\ngrad_steps = 10\nskip = 2\n\n# Class Names\nfeature_names = [\"5oClockShadow\", \"ArchedEyebrows\", \"Attractive\", \"BagsUnderEyes\", \"Bald\", \"Bangs\",\n\"BigLips\", \"BigNose\", \"BlackHair\", \"BlondHair\", \"Blurry\", \"BrownHair\", \"BushyEyebrows\", \"Chubby\",\n\"DoubleChin\", \"Eyeglasses\", \"Goatee\", \"GrayHair\", \"HeavyMakeup\", \"HighCheekbones\", \"Male\", \n\"MouthSlightlyOpen\", \"Mustache\", \"NarrowEyes\", \"NoBeard\", \"OvalFace\", \"PaleSkin\", \"PointyNose\", \n\"RecedingHairline\", \"RosyCheeks\", \"Sideburn\", \"Smiling\", \"StraightHair\", \"WavyHair\", \"WearingEarrings\", \n\"WearingHat\", \"WearingLipstick\", \"WearingNecklace\", \"WearingNecktie\", \"Young\"]\n\n# You can change this\ntarget_indices = feature_names.index(\"BlondHair\")\n\nnoise = get_noise(n_images, z_dim, device).requires_grad_()\nfor i in range(grad_steps):\n    opt.zero_grad()\n    fake_img = gen(noise)\n    fake_image_history += [fake]\n    fake_classes_score = classifier(fake_img)[:, target_indices].mean()\n    fake_classes_score.backward()\n    noise.data = calculate_updated_noise(noise, 1/grad_steps)\n    \nplt.rcParams['figure.figsize'] = [n_images * 2, grad_steps * 2]\nshow_tensor_images(torch.cat(fake_image_history[::skip], dim=2), num_images=n_images, nrow=n_images)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Entanglement and Regularization**","metadata":{}},{"cell_type":"code","source":"def get_score(current_classifications, original_classifications, target_indices, other_indices, penalty_weight):\n    other_distances = current_classifications[:, other_distances] - original_classifications[:, other_indices]\n    other_class_penalty = -torch.norm(other_distances, dim=1).mean()*penalty_weight\n    target_score = current_classifications[:, target_indices].mean()\n    \n    return target_score + other_class_penalty","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fake_image_history = []\ntarget_indices = feature_names.index(\"WearingHat\")\nother_indices = [cur_idx != target_indices for cur_idx, _ in enumerate(feature_names)]\nnoise = get_noise(n_images, z_dim, device).requires_grad_()\noriginal_classifications = classifier(gen(noise)).detach()\n\nfor i in range(len(grad_steps)):\n    opt.zero_grad()\n    fake_image = gen(noise)\n    fake_image_history += [fake_image]\n    fake_score = get_score(\n        classifier(fake_image),\n        original_classifications,\n        target_indices,\n        other_indices,\n        penalty_weight=0.1\n    )\n    fake_score.backward()\n    noise.data = calculate_updated_noise(noise, 1/grad_steps)","metadata":{},"execution_count":null,"outputs":[]}]}